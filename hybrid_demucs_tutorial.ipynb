{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSss5yu6cP4g"
   },
   "source": [
    "\n",
    "# Music Source Separation with Hybrid Demucs\n",
    "\n",
    "**Author**: [Sean Kim](https://github.com/skim0514)_\n",
    "\n",
    "This tutorial shows how to use the Hybrid Demucs model in order to\n",
    "perform music separation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MR2E8Kg8cP4i"
   },
   "source": [
    "## 1. Overview\n",
    "\n",
    "Performing music separation is composed of the following steps\n",
    "\n",
    "1. Build the Hybrid Demucs pipeline.\n",
    "2. Format the waveform into chunks of expected sizes and loop through\n",
    "   chunks (with overlap) and feed into pipeline.\n",
    "3. Collect output chunks and combine according to the way they have been\n",
    "   overlapped.\n",
    "\n",
    "The Hybrid Demucs [[DÃ©fossez, 2021](https://arxiv.org/abs/2111.03600)_]\n",
    "model is a developed version of the\n",
    "[Demucs](https://github.com/facebookresearch/demucs)_ model, a\n",
    "waveform based model which separates music into its\n",
    "respective sources, such as vocals, bass, and drums.\n",
    "Hybrid Demucs effectively uses spectrogram to learn\n",
    "through the frequency domain and also moves to time convolutions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OyZLxoEYcP4i"
   },
   "source": [
    "## 2. Preparation\n",
    "\n",
    "First, we install the necessary dependencies. The first requirement is\n",
    "``torchaudio`` and ``torch``\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83LZk8vBcP4i",
    "outputId": "7e78500f-2acc-44d6-f170-1abb452fc7fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n",
      "2.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soKb5DqIcP4j"
   },
   "source": [
    "In addition to ``torchaudio``, ``mir_eval`` is required to perform\n",
    "signal-to-distortion ratio (SDR) calculations. To install ``mir_eval``\n",
    "please use ``pip3 install mir_eval``.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oly_x0YbcP4j",
    "outputId": "967290a1-16df-4fd9-d6cf-54d404b19e8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mir_eval in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.7)\n",
      "Requirement already satisfied: numpy>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from mir_eval) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from mir_eval) (1.14.0)\n",
      "Requirement already satisfied: future in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from mir_eval) (1.0.0)\n",
      "Requirement already satisfied: six in /Users/yusufafifi/Library/Python/3.11/lib/python/site-packages (from mir_eval) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mir_eval\n",
    "from IPython.display import Audio\n",
    "from mir_eval import separation\n",
    "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB_PLUS\n",
    "from torchaudio.utils import download_asset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7gVWHo-cP4j"
   },
   "source": [
    "## 3. Construct the pipeline\n",
    "\n",
    "Pre-trained model weights and related pipeline components are bundled as\n",
    ":py:func:`torchaudio.pipelines.HDEMUCS_HIGH_MUSDB_PLUS`. This is a\n",
    ":py:class:`torchaudio.models.HDemucs` model trained on\n",
    "[MUSDB18-HQ](https://zenodo.org/record/3338373)_ and additional\n",
    "internal extra training data.\n",
    "This specific model is suited for higher sample rates, around 44.1 kHZ\n",
    "and has a nfft value of 4096 with a depth of 6 in the model implementation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-PBYjumfcP4j",
    "outputId": "db21c49f-f2f1-4fc0-f7b1-915869decee2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample rate: 44100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchaudio/pipelines/_source_separation_pipeline.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path)\n"
     ]
    }
   ],
   "source": [
    "bundle = HDEMUCS_HIGH_MUSDB_PLUS\n",
    "\n",
    "model = bundle.get_model()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "sample_rate = bundle.sample_rate\n",
    "\n",
    "print(f\"Sample rate: {sample_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIjF-FnlcP4j"
   },
   "source": [
    "## 4. Configure the application function\n",
    "\n",
    "Because ``HDemucs`` is a large and memory-consuming model it is\n",
    "very difficult to have sufficient memory to apply the model to\n",
    "an entire song at once. To work around this limitation,\n",
    "obtain the separated sources of a full song by\n",
    "chunking the song into smaller segments and run through the\n",
    "model piece by piece, and then rearrange back together.\n",
    "\n",
    "When doing this, it is important to ensure some\n",
    "overlap between each of the chunks, to accommodate for artifacts at the\n",
    "edges. Due to the nature of the model, sometimes the edges have\n",
    "inaccurate or undesired sounds included.\n",
    "\n",
    "We provide a sample implementation of chunking and arrangement below. This\n",
    "implementation takes an overlap of 1 second on each side, and then does\n",
    "a linear fade in and fade out on each side. Using the faded overlaps, I\n",
    "add these segments together, to ensure a constant volume throughout.\n",
    "This accommodates for the artifacts by using less of the edges of the\n",
    "model outputs.\n",
    "\n",
    "<img src=\"https://download.pytorch.org/torchaudio/tutorial-assets/HDemucs_Drawing.jpg\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DPj1oUqBcP4j"
   },
   "outputs": [],
   "source": [
    "from torchaudio.transforms import Fade\n",
    "\n",
    "\n",
    "def separate_sources(\n",
    "    model,\n",
    "    mix,\n",
    "    segment=10.0,\n",
    "    overlap=0.1,\n",
    "    device=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply model to a given mixture. Use fade, and add segments together in order to add model segment by segment.\n",
    "\n",
    "    Args:\n",
    "        segment (int): segment length in seconds\n",
    "        device (torch.device, str, or None): if provided, device on which to\n",
    "            execute the computation, otherwise `mix.device` is assumed.\n",
    "            When `device` is different from `mix.device`, only local computations will\n",
    "            be on `device`, while the entire tracks will be stored on `mix.device`.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = mix.device\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "\n",
    "    batch, channels, length = mix.shape\n",
    "\n",
    "    chunk_len = int(sample_rate * segment * (1 + overlap))\n",
    "    start = 0\n",
    "    end = chunk_len\n",
    "    overlap_frames = overlap * sample_rate\n",
    "    fade = Fade(fade_in_len=0, fade_out_len=int(overlap_frames), fade_shape=\"linear\")\n",
    "\n",
    "    final = torch.zeros(batch, len(model.sources), channels, length, device=device)\n",
    "\n",
    "    while start < length - overlap_frames:\n",
    "        chunk = mix[:, :, start:end]\n",
    "        with torch.no_grad():\n",
    "            out = model.forward(chunk)\n",
    "        out = fade(out)\n",
    "        final[:, :, :, start:end] += out\n",
    "        if start == 0:\n",
    "            fade.fade_in_len = int(overlap_frames)\n",
    "            start += int(chunk_len - overlap_frames)\n",
    "        else:\n",
    "            start += chunk_len\n",
    "        end += chunk_len\n",
    "        if end >= length:\n",
    "            fade.fade_out_len = 0\n",
    "    return final\n",
    "\n",
    "\n",
    "def plot_spectrogram(stft, title=\"Spectrogram\"):\n",
    "    magnitude = stft.abs()\n",
    "    spectrogram = 20 * torch.log10(magnitude + 1e-8).numpy()\n",
    "    _, axis = plt.subplots(1, 1)\n",
    "    axis.imshow(spectrogram, cmap=\"viridis\", vmin=-60, vmax=0, origin=\"lower\", aspect=\"auto\")\n",
    "    axis.set_title(title)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ba2G0m0cP4j"
   },
   "source": [
    "## 5. Run Model\n",
    "\n",
    "Finally, we run the model and store the separate source files in a\n",
    "directory\n",
    "\n",
    "As a test song, we will be using A Classic Education by NightOwl from\n",
    "MedleyDB (Creative Commons BY-NC-SA 4.0). This is also located in\n",
    "[MUSDB18-HQ](https://zenodo.org/record/3338373)_ dataset within\n",
    "the ``train`` sources.\n",
    "\n",
    "In order to test with a different song, the variable names and urls\n",
    "below can be changed alongside with the parameters to test the song\n",
    "separator in different ways.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pJWSOFnEcP4j",
    "outputId": "331d26f6-00c8-44c5-a3a9-16bae85d9e81"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'verify_mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m      5\u001b[0m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39minstall_opener(\n\u001b[1;32m      6\u001b[0m     urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbuild_opener(\n\u001b[1;32m      7\u001b[0m         urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mHTTPSHandler(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     )\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# We download the audio file from our storage. Feel free to download another file and use audio from a specific path\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m SAMPLE_SONG \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_asset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtutorial-assets/hdemucs_mix.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m waveform, sample_rate \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mload(SAMPLE_SONG)  \u001b[38;5;66;03m# replace SAMPLE_SONG with desired path for different song\u001b[39;00m\n\u001b[1;32m     16\u001b[0m waveform \u001b[38;5;241m=\u001b[39m waveform\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchaudio/utils/download.py:75\u001b[0m, in \u001b[0;36mdownload_asset\u001b[0;34m(key, hash, path, progress)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     _LG\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, key, path)\n\u001b[0;32m---> 75\u001b[0m     \u001b[43m_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhash\u001b[39m:\n\u001b[1;32m     78\u001b[0m     _LG\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVerifying the hash value.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchaudio/utils/download.py:21\u001b[0m, in \u001b[0;36m_download\u001b[0;34m(key, path, progress)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_download\u001b[39m(key, path, progress):\n\u001b[1;32m     20\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://download.pytorch.org/torchaudio/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mdownload_url_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/hub.py:708\u001b[0m, in \u001b[0;36mdownload_url_to_file\u001b[0;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[1;32m    706\u001b[0m file_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    707\u001b[0m req \u001b[38;5;241m=\u001b[39m Request(url, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.hub\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m--> 708\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m meta \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39minfo()\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(meta, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[1;32m    518\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[0;32m--> 519\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[1;32m    522\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    535\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m--> 536\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m    537\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[0;32m-> 1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:1317\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m URLError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno host given\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;66;03m# will parse host:port\u001b[39;00m\n\u001b[0;32m-> 1317\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[43mhttp_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhttp_conn_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1318\u001b[0m h\u001b[38;5;241m.\u001b[39mset_debuglevel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_debuglevel)\n\u001b[1;32m   1320\u001b[0m headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(req\u001b[38;5;241m.\u001b[39munredirected_hdrs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1428\u001b[0m, in \u001b[0;36mHTTPSConnection.__init__\u001b[0;34m(self, host, port, key_file, cert_file, timeout, source_address, context, check_hostname, blocksize)\u001b[0m\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mpost_handshake_auth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1427\u001b[0m         context\u001b[38;5;241m.\u001b[39mpost_handshake_auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1428\u001b[0m will_verify \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverify_mode\u001b[49m \u001b[38;5;241m!=\u001b[39m ssl\u001b[38;5;241m.\u001b[39mCERT_NONE\n\u001b[1;32m   1429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_hostname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1430\u001b[0m     check_hostname \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mcheck_hostname\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'verify_mode'"
     ]
    }
   ],
   "source": [
    "import certifi\n",
    "import urllib.request\n",
    "\n",
    "# Configure urllib to use certifi certificates\n",
    "urllib.request.install_opener(\n",
    "    urllib.request.build_opener(\n",
    "        urllib.request.HTTPSHandler(\n",
    "            context=certifi.where()\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# We download the audio file from our storage. Feel free to download another file and use audio from a specific path\n",
    "SAMPLE_SONG = download_asset(\"tutorial-assets/hdemucs_mix.wav\")\n",
    "waveform, sample_rate = torchaudio.load(SAMPLE_SONG)  # replace SAMPLE_SONG with desired path for different song\n",
    "waveform = waveform.to(device)\n",
    "mixture = waveform\n",
    "\n",
    "# parameters\n",
    "segment: int = 10\n",
    "overlap = 0.1\n",
    "\n",
    "print(\"Separating track\")\n",
    "\n",
    "ref = waveform.mean(0)\n",
    "waveform = (waveform - ref.mean()) / ref.std()  # normalization\n",
    "\n",
    "sources = separate_sources(\n",
    "    model,\n",
    "    waveform[None],\n",
    "    device=device,\n",
    "    segment=segment,\n",
    "    overlap=overlap,\n",
    ")[0]\n",
    "sources = sources * ref.std() + ref.mean()\n",
    "\n",
    "sources_list = model.sources\n",
    "sources = list(sources)\n",
    "\n",
    "audios = dict(zip(sources_list, sources))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hN245ijvcP4k"
   },
   "source": [
    "### 5.1 Separate Track\n",
    "\n",
    "The default set of pretrained weights that has been loaded has 4 sources\n",
    "that it is separated into: drums, bass, other, and vocals in that order.\n",
    "They have been stored into the dict âaudiosâ and therefore can be\n",
    "accessed there. For the four sources, there is a separate cell for each,\n",
    "that will create the audio, the spectrogram graph, and also calculate\n",
    "the SDR score. SDR is the signal-to-distortion\n",
    "ratio, essentially a representation to the âqualityâ of an audio track.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aw4CFc4CcP4k"
   },
   "outputs": [],
   "source": [
    "N_FFT = 4096\n",
    "N_HOP = 4\n",
    "stft = torchaudio.transforms.Spectrogram(\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=N_HOP,\n",
    "    power=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIfryHULcP4k"
   },
   "source": [
    "### 5.2 Audio Segmenting and Processing\n",
    "\n",
    "Below is the processing steps and segmenting 5 seconds of the tracks in\n",
    "order to feed into the spectrogram and to caclulate the respective SDR\n",
    "scores.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03qRsW1xcP4k"
   },
   "outputs": [],
   "source": [
    "def output_results(original_source: torch.Tensor, predicted_source: torch.Tensor, source: str):\n",
    "    print(\n",
    "        \"SDR score is:\",\n",
    "        separation.bss_eval_sources(original_source.detach().numpy(), predicted_source.detach().numpy())[0].mean(),\n",
    "    )\n",
    "    plot_spectrogram(stft(predicted_source)[0], f\"Spectrogram - {source}\")\n",
    "    return Audio(predicted_source, rate=sample_rate)\n",
    "\n",
    "\n",
    "segment_start = 150\n",
    "segment_end = 155\n",
    "\n",
    "frame_start = segment_start * sample_rate\n",
    "frame_end = segment_end * sample_rate\n",
    "\n",
    "drums_original = download_asset(\"tutorial-assets/hdemucs_drums_segment.wav\")\n",
    "bass_original = download_asset(\"tutorial-assets/hdemucs_bass_segment.wav\")\n",
    "vocals_original = download_asset(\"tutorial-assets/hdemucs_vocals_segment.wav\")\n",
    "other_original = download_asset(\"tutorial-assets/hdemucs_other_segment.wav\")\n",
    "\n",
    "drums_spec = audios[\"drums\"][:, frame_start:frame_end].cpu()\n",
    "drums, sample_rate = torchaudio.load(drums_original)\n",
    "\n",
    "bass_spec = audios[\"bass\"][:, frame_start:frame_end].cpu()\n",
    "bass, sample_rate = torchaudio.load(bass_original)\n",
    "\n",
    "vocals_spec = audios[\"vocals\"][:, frame_start:frame_end].cpu()\n",
    "vocals, sample_rate = torchaudio.load(vocals_original)\n",
    "\n",
    "other_spec = audios[\"other\"][:, frame_start:frame_end].cpu()\n",
    "other, sample_rate = torchaudio.load(other_original)\n",
    "\n",
    "mix_spec = mixture[:, frame_start:frame_end].cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4a0A57scP4k"
   },
   "source": [
    "### 5.3 Spectrograms and Audio\n",
    "\n",
    "In the next 5 cells, you can see the spectrograms with the respective\n",
    "audios. The audios can be clearly visualized using the spectrogram.\n",
    "\n",
    "The mixture clip comes from the original track, and the remaining\n",
    "tracks are the model output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RhSn8lg9cP4k"
   },
   "outputs": [],
   "source": [
    "# Mixture Clip\n",
    "plot_spectrogram(stft(mix_spec)[0], \"Spectrogram - Mixture\")\n",
    "Audio(mix_spec, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avWLMsT3cP4k"
   },
   "source": [
    "Drums SDR, Spectrogram, and Audio\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBOkuD2hcP4k"
   },
   "outputs": [],
   "source": [
    "# Drums Clip\n",
    "output_results(drums, drums_spec, \"drums\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIxBYh1LcP4k"
   },
   "source": [
    "Bass SDR, Spectrogram, and Audio\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKmyZpbBcP4k"
   },
   "outputs": [],
   "source": [
    "# Bass Clip\n",
    "output_results(bass, bass_spec, \"bass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZCNSr_1cP4k"
   },
   "source": [
    "Vocals SDR, Spectrogram, and Audio\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aVIOVNKbcP4l"
   },
   "outputs": [],
   "source": [
    "# Vocals Audio\n",
    "output_results(vocals, vocals_spec, \"vocals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yu68QYkLcP4l"
   },
   "source": [
    "Other SDR, Spectrogram, and Audio\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gRNTyynYcP4l"
   },
   "outputs": [],
   "source": [
    "# Other Clip\n",
    "output_results(other, other_spec, \"other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6pJA5YwcP4l"
   },
   "outputs": [],
   "source": [
    "# Optionally, the full audios can be heard in from running the next 5\n",
    "# cells. They will take a bit longer to load, so to run simply uncomment\n",
    "# out the ``Audio`` cells for the respective track to produce the audio\n",
    "# for the full song.\n",
    "#\n",
    "\n",
    "# Full Audio\n",
    "# Audio(mixture, rate=sample_rate)\n",
    "\n",
    "# Drums Audio\n",
    "# Audio(audios[\"drums\"], rate=sample_rate)\n",
    "\n",
    "# Bass Audio\n",
    "# Audio(audios[\"bass\"], rate=sample_rate)\n",
    "\n",
    "# Vocals Audio\n",
    "# Audio(audios[\"vocals\"], rate=sample_rate)\n",
    "\n",
    "# Other Audio\n",
    "# Audio(audios[\"other\"], rate=sample_rate)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
