{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: whisper in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.1.10)\n",
      "Requirement already satisfied: six in /Users/yusufafifi/Library/Python/3.11/lib/python/site-packages (from whisper) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyaudio in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.2.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install whisper\n",
    "%pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import whisper\n",
    "import numpy as np\n",
    "\n",
    "# Load the Whisper model\n",
    "model = whisper.load_model(\"small.en\")  \n",
    "\n",
    "# Audio settings\n",
    "CHUNK = 1024  # Number of audio frames per buffer\n",
    "FORMAT = pyaudio.paInt16  # Audio format\n",
    "CHANNELS = 1  # Mono audio\n",
    "RATE = 16000  # Sampling rate (must match Whisper's requirement)\n",
    "\n",
    "# Initialize PyAudio\n",
    "p = pyaudio.PyAudio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device 0: MacBook Pro Microphone\n",
      "Device 1: MacBook Pro Speakers\n",
      "Device 2: Microsoft Teams Audio\n",
      "Device 3: ZoomAudioDevice\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "p = pyaudio.PyAudio()\n",
    "for i in range(p.get_device_count()):\n",
    "    print(f\"Device {i}: {p.get_device_info_by_index(i)['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bye.\n",
      "\n",
      " It's fine.\n",
      " Twinkle.\n",
      " You are\n",
      " about\n",
      " time and\n",
      " Good sleep\n",
      " The little st-\n",
      "\n",
      " One, two, one.\n",
      " you are.\n",
      "\n",
      "\n",
      "Stopping the transcription\n"
     ]
    }
   ],
   "source": [
    "import queue\n",
    "\n",
    "\n",
    "audio_queue = queue.Queue()\n",
    "stream = p.open(\n",
    "    format=FORMAT,\n",
    "    channels=CHANNELS,\n",
    "    rate=RATE,\n",
    "    input=True,\n",
    "    input_device_index=0,  \n",
    "    frames_per_buffer=CHUNK\n",
    ")\n",
    "\n",
    "try:\n",
    "  while True:\n",
    "      audio_data = stream.read(CHUNK, exception_on_overflow=False)\n",
    "      audio_queue.put(audio_data)\n",
    "      \n",
    "      # Process transcription after collecting enough audio\n",
    "      if audio_queue.qsize() >= 2 * RATE // CHUNK:  # Process ~1 second of audio\n",
    "          audio_buffer = b''.join([audio_queue.get() for _ in range(audio_queue.qsize())])\n",
    "          audio_array = np.frombuffer(audio_buffer, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "          \n",
    "          result = model.transcribe(audio_array, fp16=False)\n",
    "          print(result[\"text\"])\n",
    "except:\n",
    "    print(\"Stopping the transcription\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Last transcriptions:\n",
      "[09:41:36]  Twinkle, twinkle,\n",
      "[09:41:38]  The Heatha briefing results classed\n",
      "[09:41:40]  ow\n",
      "[09:41:41]  on what you...\n"
     ]
    }
   ],
   "source": [
    "import queue\n",
    "import numpy as np\n",
    "import pyaudio\n",
    "import whisper\n",
    "import threading\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import wave\n",
    "import datetime\n",
    "\n",
    "class LiveTranscriber:\n",
    "    def __init__(self, model_name=\"base.en\", device=\"cpu\"):\n",
    "        # Audio parameters\n",
    "        self.FORMAT = pyaudio.paFloat32\n",
    "        self.CHANNELS = 1\n",
    "        self.RATE = 16000\n",
    "        self.CHUNK = 1024\n",
    "        self.SILENCE_THRESHOLD = 0.01\n",
    "        \n",
    "        # Initialize components\n",
    "        self.model = whisper.load_model(model_name).to(device)\n",
    "        self.audio_queue = queue.Queue()\n",
    "        self.is_running = False\n",
    "        self.transcriptions = []\n",
    "        \n",
    "        # Setup PyAudio\n",
    "        self.p = pyaudio.PyAudio()\n",
    "        self._setup_stream()\n",
    "        \n",
    "    def _setup_stream(self):\n",
    "        \"\"\"Initialize audio stream with error handling\"\"\"\n",
    "        try:\n",
    "            self.stream = self.p.open(\n",
    "                format=self.FORMAT,\n",
    "                channels=self.CHANNELS,\n",
    "                rate=self.RATE,\n",
    "                input=True,\n",
    "                input_device_index=0,\n",
    "                frames_per_buffer=self.CHUNK\n",
    "            )\n",
    "        except OSError as e:\n",
    "            print(f\"Error setting up audio stream: {e}\")\n",
    "            self.p.terminate()\n",
    "            raise\n",
    "            \n",
    "    def _process_audio(self):\n",
    "        \"\"\"Process audio buffer and transcribe\"\"\"\n",
    "        while self.is_running:\n",
    "            if self.audio_queue.qsize() >= 1.5*self.RATE // self.CHUNK:\n",
    "                # Get audio buffer\n",
    "                audio_buffer = b''.join([self.audio_queue.get() for _ in range(self.audio_queue.qsize())])\n",
    "                audio_array = np.frombuffer(audio_buffer, dtype=np.float32)\n",
    "                \n",
    "                # Check if audio contains speech\n",
    "                if np.abs(audio_array).mean() > self.SILENCE_THRESHOLD:\n",
    "                    try:\n",
    "                        result = self.model.transcribe(audio_array, fp16=False)\n",
    "                        if result[\"text\"].strip():\n",
    "                            timestamp = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "                            self.transcriptions.append(f\"[{timestamp}] {result['text']}\")\n",
    "                            \n",
    "                            # Display last 5 transcriptions\n",
    "                            clear_output(wait=True)\n",
    "                            print(\"\\nLast transcriptions:\")\n",
    "                            for t in self.transcriptions:\n",
    "                                print(t)\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Transcription error: {e}\")\n",
    "                \n",
    "    def start(self):\n",
    "        \"\"\"Start transcription\"\"\"\n",
    "        self.is_running = True\n",
    "        self.processor_thread = threading.Thread(target=self._process_audio)\n",
    "        self.processor_thread.start()\n",
    "        \n",
    "        try:\n",
    "            while self.is_running:\n",
    "                audio_data = self.stream.read(self.CHUNK, exception_on_overflow=False)\n",
    "                self.audio_queue.put(audio_data)\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            self.stop()\n",
    "            \n",
    "    def stop(self):\n",
    "        \"\"\"Stop transcription and cleanup\"\"\"\n",
    "        self.is_running = False\n",
    "        if hasattr(self, 'processor_thread'):\n",
    "            self.processor_thread.join()\n",
    "        \n",
    "        self.stream.stop_stream()\n",
    "        self.stream.close()\n",
    "        self.p.terminate()\n",
    "        \n",
    "        # Save transcriptions\n",
    "        with open(\"output/transcription_log.txt\", \"w\") as f:\n",
    "            f.write(\"\\n\".join(self.transcriptions))\n",
    "            \n",
    "# Usage\n",
    "transcriber = LiveTranscriber(model_name=\"base.en\")\n",
    "try:\n",
    "    transcriber.start()\n",
    "except KeyboardInterrupt:\n",
    "    transcriber.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Last transcriptions:\n",
      "[17:25:56]  you\n",
      "[17:25:58]  what\n",
      "[17:25:59]  I.\n",
      "[17:26:01]  the\n",
      "[17:26:02]  are.\n"
     ]
    }
   ],
   "source": [
    "from whisper.tokenizer import get_tokenizer\n",
    "from rapidfuzz import fuzz, process\n",
    "\n",
    "class LiveTranscriber:\n",
    "    def __init__(self, model_name=\"base.en\", device=\"cpu\", similarity_threshold=80):\n",
    "        # Audio parameters\n",
    "        self.FORMAT = pyaudio.paFloat32\n",
    "        self.CHANNELS = 1\n",
    "        self.RATE = 16000\n",
    "        self.CHUNK = 1024\n",
    "        self.SILENCE_THRESHOLD = 0.01\n",
    "        self.similarity_threshold = similarity_threshold  # Minimum similarity percentage\n",
    "\n",
    "        # Initialize components\n",
    "        self.model = whisper.load_model(model_name).to(device)\n",
    "        self.tokenizer = get_tokenizer(model_name)  # Initialize tokenizer\n",
    "        self.audio_queue = queue.Queue()\n",
    "        self.is_running = False\n",
    "        self.transcriptions = []\n",
    "        \n",
    "        # Prepare lyrics for matching\n",
    "        lyrics = \"Twinkle, twinkle, little star, How I wonder what you are! Up above the world so high, Like a diamond in the sky.\"\n",
    "        self.lyrics_tokens = self.tokenizer.encode(lyrics)  # Tokenize lyrics\n",
    "        \n",
    "        # Setup PyAudio\n",
    "        self.p = pyaudio.PyAudio()\n",
    "        self._setup_stream()\n",
    "        \n",
    "    def _setup_stream(self):\n",
    "        \"\"\"Initialize audio stream with error handling\"\"\"\n",
    "        try:\n",
    "            self.stream = self.p.open(\n",
    "                format=self.FORMAT,\n",
    "                channels=self.CHANNELS,\n",
    "                rate=self.RATE,\n",
    "                input=True,\n",
    "                input_device_index=0,\n",
    "                frames_per_buffer=self.CHUNK\n",
    "            )\n",
    "        except OSError as e:\n",
    "            print(f\"Error setting up audio stream: {e}\")\n",
    "            self.p.terminate()\n",
    "            raise\n",
    "            \n",
    "    def _process_audio(self):\n",
    "        \"\"\"Process audio buffer and transcribe\"\"\"\n",
    "        while self.is_running:\n",
    "            if self.audio_queue.qsize() >= 1.5 * self.RATE // self.CHUNK:\n",
    "                # Get audio buffer\n",
    "                audio_buffer = b''.join([self.audio_queue.get() for _ in range(self.audio_queue.qsize())])\n",
    "                audio_array = np.frombuffer(audio_buffer, dtype=np.float32)\n",
    "                \n",
    "                # Check if audio contains speech\n",
    "                if np.abs(audio_array).mean() > self.SILENCE_THRESHOLD:\n",
    "                    try:\n",
    "                        result = self.model.transcribe(audio_array, fp16=False)\n",
    "                        if result[\"text\"].strip():\n",
    "                            transcript = result[\"text\"]\n",
    "                            matching_tokens = self._match_tokens(transcript)\n",
    "                            \n",
    "                            if matching_tokens:\n",
    "                                timestamp = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "                                matched_text = self.tokenizer.decode(matching_tokens)\n",
    "                                self.transcriptions.append(f\"[{timestamp}] {matched_text}\")\n",
    "                                \n",
    "                                # Display last 5 transcriptions\n",
    "                                clear_output(wait=True)\n",
    "                                print(\"\\nLast transcriptions:\")\n",
    "                                for t in self.transcriptions[-5:]:\n",
    "                                    print(t)\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Transcription error: {e}\")\n",
    "                        \n",
    "    def _match_tokens(self, transcript):\n",
    "        \"\"\"Match tokens from the transcript to the lyrics with fuzzy matching\"\"\"\n",
    "        # Tokenize the transcription text\n",
    "        transcript_tokens = self.tokenizer.encode(transcript)\n",
    "        matched_tokens = []\n",
    "\n",
    "        for token in transcript_tokens:\n",
    "            # Decode the token into a string\n",
    "            token_text = self.tokenizer.decode([token]).strip()\n",
    "\n",
    "            # Perform fuzzy matching on the string representation\n",
    "            match = process.extractOne(\n",
    "                token_text, [self.tokenizer.decode([lt]).strip() for lt in self.lyrics_tokens], scorer=fuzz.ratio\n",
    "            )\n",
    "\n",
    "            if match and match[1] >= self.similarity_threshold:  # Ensure similarity meets threshold\n",
    "                matched_tokens.append(token)\n",
    "\n",
    "        return matched_tokens if matched_tokens else None\n",
    "                \n",
    "    def start(self):\n",
    "        \"\"\"Start transcription\"\"\"\n",
    "        self.is_running = True\n",
    "        self.processor_thread = threading.Thread(target=self._process_audio)\n",
    "        self.processor_thread.start()\n",
    "        \n",
    "        try:\n",
    "            while self.is_running:\n",
    "                audio_data = self.stream.read(self.CHUNK, exception_on_overflow=False)\n",
    "                self.audio_queue.put(audio_data)\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            self.stop()\n",
    "            \n",
    "    def stop(self):\n",
    "        \"\"\"Stop transcription and cleanup\"\"\"\n",
    "        self.is_running = False\n",
    "        if hasattr(self, 'processor_thread'):\n",
    "            self.processor_thread.join()\n",
    "        \n",
    "        self.stream.stop_stream()\n",
    "        self.stream.close()\n",
    "        self.p.terminate()\n",
    "        \n",
    "        # Save transcriptions\n",
    "        with open(\"output/transcription_log.txt\", \"w\") as f:\n",
    "            f.write(\"\\n\".join(self.transcriptions))\n",
    "\n",
    "transcriber = LiveTranscriber(model_name=\"base.en\")\n",
    "try:\n",
    "    transcriber.start()\n",
    "except KeyboardInterrupt:\n",
    "    transcriber.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_currently_playing_track(access_token):\n",
    "    url = \"https://api.spotify.com/v1/me/player/currently-playing\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {access_token}\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        track_name = data['item']['name']\n",
    "        artist_name = data['item']['artists'][0]['name']\n",
    "        return track_name, artist_name\n",
    "    else:\n",
    "        print(\"No track is currently playing or error occurred.\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_transcription_to_lyrics(transcription, lyrics, threshold=80):\n",
    "    for line in lyrics.split(\"\\n\"):\n",
    "        if fuzz.partial_ratio(transcription.lower(), line.lower()) >= threshold:\n",
    "            return f\"Match found in lyrics: {line}\"\n",
    "    return \"No match found in lyrics.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
