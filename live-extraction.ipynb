{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: demucs in /Users/ediz/anaconda3/lib/python3.11/site-packages (4.0.1)\n",
      "Requirement already satisfied: dora-search in /Users/ediz/anaconda3/lib/python3.11/site-packages (from demucs) (0.1.12)\n",
      "Requirement already satisfied: einops in /Users/ediz/anaconda3/lib/python3.11/site-packages (from demucs) (0.8.0)\n",
      "Requirement already satisfied: julius>=0.2.3 in /Users/ediz/anaconda3/lib/python3.11/site-packages (from demucs) (0.2.7)\n",
      "Requirement already satisfied: lameenc>=1.2 in /Users/ediz/anaconda3/lib/python3.11/site-packages (from demucs) (1.8.1)\n",
      "Requirement already satisfied: openunmix in /Users/ediz/anaconda3/lib/python3.11/site-packages (from demucs) (1.3.0)\n",
      "Requirement already satisfied: pyyaml in /Users/ediz/anaconda3/lib/python3.11/site-packages (from demucs) (6.0)\n",
      "Requirement already satisfied: torch>=1.8.1 in /Users/ediz/anaconda3/lib/python3.11/site-packages (from demucs) (2.5.1)\n",
      "Requirement already satisfied: torchaudio>=0.8 in /Users/ediz/anaconda3/lib/python3.11/site-packages (from demucs) (2.5.1)\n",
      "Requirement already satisfied: tqdm in /Users/ediz/anaconda3/lib/python3.11/site-packages (from demucs) (4.65.0)\n",
      "Requirement already satisfied: filelock in /Users/ediz/anaconda3/lib/python3.11/site-packages (from torch>=1.8.1->demucs) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/ediz/anaconda3/lib/python3.11/site-packages (from torch>=1.8.1->demucs) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/ediz/anaconda3/lib/python3.11/site-packages (from torch>=1.8.1->demucs) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/ediz/anaconda3/lib/python3.11/site-packages (from torch>=1.8.1->demucs) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/ediz/anaconda3/lib/python3.11/site-packages (from torch>=1.8.1->demucs) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/ediz/anaconda3/lib/python3.11/site-packages (from torch>=1.8.1->demucs) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/ediz/anaconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.8.1->demucs) (1.3.0)\n",
      "Requirement already satisfied: omegaconf in /Users/ediz/anaconda3/lib/python3.11/site-packages (from dora-search->demucs) (2.3.0)\n",
      "Requirement already satisfied: retrying in /Users/ediz/anaconda3/lib/python3.11/site-packages (from dora-search->demucs) (1.3.4)\n",
      "Requirement already satisfied: submitit in /Users/ediz/anaconda3/lib/python3.11/site-packages (from dora-search->demucs) (1.5.2)\n",
      "Requirement already satisfied: treetable in /Users/ediz/anaconda3/lib/python3.11/site-packages (from dora-search->demucs) (0.2.5)\n",
      "Requirement already satisfied: numpy in /Users/ediz/anaconda3/lib/python3.11/site-packages (from openunmix->demucs) (1.24.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ediz/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.8.1->demucs) (2.1.1)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /Users/ediz/anaconda3/lib/python3.11/site-packages (from omegaconf->dora-search->demucs) (4.9.3)\n",
      "Requirement already satisfied: six>=1.7.0 in /Users/ediz/anaconda3/lib/python3.11/site-packages (from retrying->dora-search->demucs) (1.16.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.1 in /Users/ediz/anaconda3/lib/python3.11/site-packages (from submitit->dora-search->demucs) (2.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: soundfile in /Users/ediz/anaconda3/lib/python3.11/site-packages (0.12.1)\n",
      "Requirement already satisfied: cffi>=1.0 in /Users/ediz/anaconda3/lib/python3.11/site-packages (from soundfile) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /Users/ediz/anaconda3/lib/python3.11/site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install demucs\n",
    "%pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "from mir_eval import separation\n",
    "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB_PLUS\n",
    "from torchaudio.utils import download_asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample rate: 44100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ediz/anaconda3/lib/python3.11/site-packages/torchaudio/pipelines/_source_separation_pipeline.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path)\n"
     ]
    }
   ],
   "source": [
    "bundle = HDEMUCS_HIGH_MUSDB_PLUS\n",
    "\n",
    "model = bundle.get_model()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "sample_rate = bundle.sample_rate\n",
    "\n",
    "print(f\"Sample rate: {sample_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.transforms import Fade\n",
    "\n",
    "\n",
    "def separate_sources(\n",
    "    model,\n",
    "    mix,\n",
    "    segment=10.0,\n",
    "    overlap=0.1,\n",
    "    device=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply model to a given mixture. Use fade, and add segments together in order to add model segment by segment.\n",
    "\n",
    "    Args:\n",
    "        segment (int): segment length in seconds\n",
    "        device (torch.device, str, or None): if provided, device on which to\n",
    "            execute the computation, otherwise `mix.device` is assumed.\n",
    "            When `device` is different from `mix.device`, only local computations will\n",
    "            be on `device`, while the entire tracks will be stored on `mix.device`.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = mix.device\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "\n",
    "    batch, channels, length = mix.shape\n",
    "\n",
    "    chunk_len = int(sample_rate * segment * (1 + overlap))\n",
    "    start = 0\n",
    "    end = chunk_len\n",
    "    overlap_frames = overlap * sample_rate\n",
    "    fade = Fade(fade_in_len=0, fade_out_len=int(overlap_frames), fade_shape=\"linear\")\n",
    "\n",
    "    final = torch.zeros(batch, len(model.sources), channels, length, device=device)\n",
    "\n",
    "    while start < length - overlap_frames:\n",
    "        chunk = mix[:, :, start:end]\n",
    "        with torch.no_grad():\n",
    "            out = model.forward(chunk)\n",
    "        out = fade(out)\n",
    "        final[:, :, :, start:end] += out\n",
    "        if start == 0:\n",
    "            fade.fade_in_len = int(overlap_frames)\n",
    "            start += int(chunk_len - overlap_frames)\n",
    "        else:\n",
    "            start += chunk_len\n",
    "        end += chunk_len\n",
    "        if end >= length:\n",
    "            fade.fade_out_len = 0\n",
    "    return final\n",
    "\n",
    "\n",
    "def plot_spectrogram(stft, title=\"Spectrogram\"):\n",
    "    magnitude = stft.abs()\n",
    "    spectrogram = 20 * torch.log10(magnitude + 1e-8).numpy()\n",
    "    _, axis = plt.subplots(1, 1)\n",
    "    axis.imshow(spectrogram, cmap=\"viridis\", vmin=-60, vmax=0, origin=\"lower\", aspect=\"auto\")\n",
    "    axis.set_title(title)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We download the audio file from our storage. Feel free to download another file and use audio from a specific path\n",
    "SAMPLE_SONG = download_asset(\"tutorial-assets/hdemucs_mix.wav\")\n",
    "waveform, sample_rate = torchaudio.load(SAMPLE_SONG)  # replace SAMPLE_SONG with desired path for different song\n",
    "waveform = waveform.to(device)\n",
    "mixture = waveform\n",
    "\n",
    "# parameters\n",
    "segment: int = 10\n",
    "overlap = 0.1\n",
    "\n",
    "print(\"Separating track\")\n",
    "\n",
    "ref = waveform.mean(0)\n",
    "waveform = (waveform - ref.mean()) / ref.std()  # normalization\n",
    "\n",
    "sources = separate_sources(\n",
    "    model,\n",
    "    waveform[None],\n",
    "    device=device,\n",
    "    segment=segment,\n",
    "    overlap=overlap,\n",
    ")[0]\n",
    "sources = sources * ref.std() + ref.mean()\n",
    "\n",
    "sources_list = model.sources\n",
    "sources = list(sources)\n",
    "\n",
    "audios = dict(zip(sources_list, sources))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import speech_recognition as sr\n",
    "from datetime import datetime, timedelta\n",
    "from queue import Queue\n",
    "from time import sleep\n",
    "import torch\n",
    "from torchaudio.transforms import Fade, Resample\n",
    "import torchaudio\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Audio recording setup\n",
    "energy_threshold = 1000\n",
    "record_timeout = 2.0\n",
    "phrase_timeout = 3.0\n",
    "phrase_time = None\n",
    "data_queue = Queue()\n",
    "recorder = sr.Recognizer()\n",
    "recorder.energy_threshold = energy_threshold\n",
    "recorder.dynamic_energy_threshold = False\n",
    "\n",
    "source = sr.Microphone(sample_rate=16000)\n",
    "\n",
    "def record_callback(_, audio: sr.AudioData) -> None:\n",
    "    data = audio.get_raw_data()\n",
    "    data_queue.put(data)\n",
    "\n",
    "# Initialize Demucs model\n",
    "bundle = HDEMUCS_HIGH_MUSDB_PLUS\n",
    "model = bundle.get_model()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model_sample_rate = bundle.sample_rate\n",
    "\n",
    "def separate_sources(model, mix, segment=10.0, overlap=0.1, device=None):\n",
    "    if device is None:\n",
    "        device = mix.device\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "\n",
    "    batch, channels, length = mix.shape\n",
    "    chunk_len = int(model_sample_rate * segment * (1 + overlap))\n",
    "    start = 0\n",
    "    end = chunk_len\n",
    "    overlap_frames = overlap * model_sample_rate\n",
    "    fade = Fade(fade_in_len=0, fade_out_len=int(overlap_frames), fade_shape=\"linear\")\n",
    "\n",
    "    final = torch.zeros(batch, len(model.sources), channels, length, device=device)\n",
    "\n",
    "    while start < length - overlap_frames:\n",
    "        chunk = mix[:, :, start:end]\n",
    "        with torch.no_grad():\n",
    "            out = model.forward(chunk)\n",
    "        out = fade(out)\n",
    "        final[:, :, :, start:end] += out\n",
    "        if start == 0:\n",
    "            fade.fade_in_len = int(overlap_frames)\n",
    "            start += int(chunk_len - overlap_frames)\n",
    "        else:\n",
    "            start += chunk_len\n",
    "        end += chunk_len\n",
    "        if end >= length:\n",
    "            fade.fade_out_len = 0\n",
    "    return final\n",
    "\n",
    "# Main processing loop\n",
    "with source:\n",
    "    recorder.adjust_for_ambient_noise(source)\n",
    "\n",
    "stop_call = recorder.listen_in_background(source, record_callback, phrase_time_limit=record_timeout)\n",
    "print(\"Model loaded and microphone initialized.\\n\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        now = datetime.utcnow()\n",
    "        if not data_queue.empty():\n",
    "            if phrase_time and now - phrase_time > timedelta(seconds=phrase_timeout):\n",
    "                phrase_complete = True\n",
    "            phrase_time = now\n",
    "\n",
    "            # Get audio data\n",
    "            audio_data = b''.join(data_queue.queue)\n",
    "            data_queue.queue.clear()\n",
    "\n",
    "            # Convert to waveform using torchaudio\n",
    "            waveform = torch.frombuffer(audio_data, dtype=torch.int16).float() / 32768.0\n",
    "            waveform = waveform.unsqueeze(0).to(device)\n",
    "\n",
    "            # Resample to model's sample rate if necessary\n",
    "            if source.SAMPLE_RATE != model_sample_rate:\n",
    "                resampler = Resample(orig_freq=source.SAMPLE_RATE, new_freq=model_sample_rate)\n",
    "                waveform = resampler(waveform)\n",
    "\n",
    "            # Ensure correct shape for Demucs model\n",
    "            if waveform.shape[0] != model.audio_channels:\n",
    "                waveform = waveform.expand(model.audio_channels, -1)\n",
    "\n",
    "            # Normalize the waveform\n",
    "            ref = waveform.mean(0)\n",
    "            waveform = (waveform - ref.mean()) / ref.std()\n",
    "\n",
    "            # Separate sources using Demucs\n",
    "            separated_sources = separate_sources(\n",
    "                model,\n",
    "                waveform.unsqueeze(0),\n",
    "                device=device,\n",
    "                segment=10,\n",
    "                overlap=0.1\n",
    "            )[0]\n",
    "\n",
    "            # Denormalize the sources\n",
    "            separated_sources = separated_sources * ref.std() + ref.mean()\n",
    "\n",
    "            # Extract vocals\n",
    "            sources_list = model.sources\n",
    "            sources = list(separated_sources)\n",
    "            audios = dict(zip(sources_list, sources))\n",
    "            vocals = audios[\"vocals\"]\n",
    "\n",
    "            # Play back the separated vocals\n",
    "            vocals_np = vocals.squeeze().cpu().numpy()\n",
    "            display(Audio(vocals_np, rate=model_sample_rate))\n",
    "\n",
    "        else:\n",
    "            sleep(0.25)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nSeparation stopped by user.\")\n",
    "    stop_call()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
