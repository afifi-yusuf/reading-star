{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and microphone initialized.\n",
      "\n",
      "Received 101376 samples for transcription\n",
      "[<openvino_genai.py_openvino_genai.WhisperDecodedResultChunk object at 0x345593e70>]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 76\u001b[0m\n\u001b[1;32m     72\u001b[0m result \u001b[38;5;241m=\u001b[39m ov_pipeline\u001b[38;5;241m.\u001b[39mgenerate(audio_np, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranscribe\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_timestamps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mchunks\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[0;32m---> 76\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m phrase_complete:\n\u001b[1;32m     79\u001b[0m     transcription\u001b[38;5;241m.\u001b[39mappend(text)\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openvino\n",
    "import openvino_genai\n",
    "import torch\n",
    "import whisper\n",
    "import numpy as np\n",
    "import speech_recognition as sr\n",
    "\n",
    "from cmd_helper import optimum_cli\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from queue import Queue\n",
    "from time import sleep\n",
    "\n",
    "# Define arguments as variables for notebook use\n",
    "non_english = False    # Use non-English model if True\n",
    "energy_threshold = 1000  # Energy level for mic detection\n",
    "record_timeout = 2.0     # Real-time recording in seconds\n",
    "phrase_timeout = 3.0     # Pause length between phrases for new line\n",
    "\n",
    "# Initialize variables\n",
    "phrase_time = None\n",
    "data_queue = Queue()\n",
    "recorder = sr.Recognizer()\n",
    "recorder.energy_threshold = energy_threshold\n",
    "recorder.dynamic_energy_threshold = False\n",
    "\n",
    "# Local organisation of models\n",
    "model_value = \"openai/whisper-medium\"\n",
    "model_dir = \"whisper-medium\"\n",
    "\n",
    "# Set up microphone source\n",
    "source = sr.Microphone(sample_rate=16000)\n",
    "\n",
    "# export the whisper model\n",
    "if not Path(model_dir).exists():\n",
    "    optimum_cli(model_value, model_dir)\n",
    "\n",
    "ov_pipeline = openvino_genai.WhisperPipeline(model_dir, device='CPU')\n",
    "\n",
    "# Initialize transcription list\n",
    "transcription = ['']\n",
    "\n",
    "# Adjust microphone for ambient noise\n",
    "with source:\n",
    "    recorder.adjust_for_ambient_noise(source)\n",
    "\n",
    "def record_callback(_, audio: sr.AudioData) -> None:\n",
    "    \"\"\"Threaded callback function to handle audio data.\"\"\"\n",
    "    data = audio.get_raw_data()\n",
    "    data_queue.put(data)\n",
    "\n",
    "# Start background recording\n",
    "recorder.listen_in_background(source, record_callback, phrase_time_limit=record_timeout)\n",
    "print(\"Model loaded and microphone initialized.\\n\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        now = datetime.utcnow()\n",
    "        if not data_queue.empty():\n",
    "            phrase_complete = False\n",
    "            if phrase_time and now - phrase_time > timedelta(seconds=phrase_timeout):\n",
    "                phrase_complete = True\n",
    "            phrase_time = now\n",
    "\n",
    "            audio_data = b''.join(data_queue.queue)\n",
    "            data_queue.queue.clear()\n",
    "\n",
    "            audio_np = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "            print(f\"Received {len(audio_np)} samples for transcription\")\n",
    "\n",
    "            result = ov_pipeline.generate(audio_np, task=\"transcribe\", return_timestamps=True).chunks\n",
    "\n",
    "            print(result)\n",
    "\n",
    "            text = result['text'].strip()\n",
    "\n",
    "            if phrase_complete:\n",
    "                transcription.append(text)\n",
    "            else:\n",
    "                transcription[-1] = text\n",
    "\n",
    "            os.system('cls' if os.name == 'nt' else 'clear')\n",
    "            for line in transcription:\n",
    "                print(line)\n",
    "            print('', end='', flush=True)\n",
    "        else:\n",
    "            sleep(0.25)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTranscription stopped by user.\")\n",
    "    print(\"\\nFinal Transcription:\")\n",
    "    for line in transcription:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuzzy matching to find closest match phrase in the current verse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import speech_recognition as sr\n",
    "import whisper\n",
    "import torch\n",
    "from datetime import datetime, timedelta\n",
    "from queue import Queue\n",
    "from time import sleep\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Known lyrics for \"Twinkle, Twinkle, Little Star\"\n",
    "lyrics = {\n",
    "    \"Verse 1\": [\n",
    "        \"Twinkle, twinkle, little star\",\n",
    "        \"How I wonder what you are\",\n",
    "        \"Up above the world so high\",\n",
    "        \"Like a diamond in the sky\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Fuzzy matching function\n",
    "def find_closest_match(transcription, lyrics):\n",
    "    best_match = \"\"\n",
    "    highest_similarity = 0\n",
    "    for line in lyrics:\n",
    "        similarity = SequenceMatcher(None, transcription, line).ratio()\n",
    "        if similarity > highest_similarity:\n",
    "            highest_similarity = similarity\n",
    "            best_match = line\n",
    "    return best_match, highest_similarity\n",
    "\n",
    "# Initialize variables for speech recognition and Whisper\n",
    "energy_threshold = 1000  # Energy level for mic detection\n",
    "record_timeout = 2.0  # Real-time recording in seconds\n",
    "phrase_timeout = 3.0  # Pause length between phrases for new line\n",
    "phrase_time = None\n",
    "data_queue = Queue()\n",
    "recorder = sr.Recognizer()\n",
    "recorder.energy_threshold = energy_threshold\n",
    "recorder.dynamic_energy_threshold = False\n",
    "\n",
    "# Set up microphone source\n",
    "source = sr.Microphone(sample_rate=16000)\n",
    "\n",
    "# Load Whisper model\n",
    "audio_model = whisper.load_model(\"base.en\")\n",
    "\n",
    "# Initialize transcription list\n",
    "transcription = ['']\n",
    "\n",
    "# Adjust microphone for ambient noise\n",
    "with source:\n",
    "    recorder.adjust_for_ambient_noise(source)\n",
    "\n",
    "# Define a callback for audio data processing\n",
    "def record_callback(_, audio: sr.AudioData) -> None:\n",
    "    \"\"\"Threaded callback function to handle audio data.\"\"\"\n",
    "    data = audio.get_raw_data()\n",
    "    data_queue.put(data)\n",
    "\n",
    "# Start background recording\n",
    "recorder.listen_in_background(source, record_callback, phrase_time_limit=record_timeout)\n",
    "print(\"Model loaded and microphone initialized.\\n\")\n",
    "\n",
    "try:\n",
    "    current_verse = \"Verse 1\"  # Start with the first verse\n",
    "    while True:\n",
    "        now = datetime.utcnow()\n",
    "        if not data_queue.empty():\n",
    "            phrase_complete = False\n",
    "            if phrase_time and now - phrase_time > timedelta(seconds=phrase_timeout):\n",
    "                phrase_complete = True\n",
    "            phrase_time = now\n",
    "\n",
    "            # Combine audio data from queue\n",
    "            audio_data = b''.join(data_queue.queue)\n",
    "            data_queue.queue.clear()\n",
    "\n",
    "            # Convert audio data to the format Whisper expects\n",
    "            audio_np = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "\n",
    "            # Perform transcription using Whisper\n",
    "            result = audio_model.transcribe(audio_np, fp16=torch.cuda.is_available())\n",
    "            recognized_text = result['text'].strip()\n",
    "\n",
    "            # Match the transcription to the current verse's lyrics\n",
    "            match, similarity = find_closest_match(recognized_text, lyrics[current_verse])\n",
    "\n",
    "            #if phrase_complete:\n",
    "             #   transcription.append(match if similarity > 0.7 else recognized_text)\n",
    "            #else:\n",
    "             #   transcription[-1] = match if similarity > 0.7 else recognized_text\n",
    "\n",
    "            # Print the transcription and matched lyrics\n",
    "            os.system('cls' if os.name == 'nt' else 'clear')\n",
    "            #print(\"Transcription (matched to lyrics):\\n\")\n",
    "            #for line in transcription:\n",
    "             #   print(line)\n",
    "            print(f\"\\nRecognized: {recognized_text}\")\n",
    "            print(f\"Best Match: {match} (Similarity: {similarity:.2f})\")\n",
    "        else:\n",
    "            sleep(0.25)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTranscription stopped by user.\")\n",
    "    print(\"\\nFinal Transcription:\")\n",
    "    for line in transcription:\n",
    "        print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
