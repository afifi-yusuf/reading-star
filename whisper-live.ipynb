{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input layer: <ConstOutput: names[input_features] shape[1,80,3000] type: f16>\n",
      "Output layer: <ConstOutput: names[last_hidden_state] shape[1,1500,768] type: f16>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openvino\n",
    "import whisper\n",
    "import numpy as np\n",
    "import speech_recognition as sr\n",
    "\n",
    "from cmd_helper import optimum_cli\n",
    "from datetime import datetime, timedelta\n",
    "from openvino.runtime import Core\n",
    "from pathlib import Path\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from queue import Queue\n",
    "from time import sleep\n",
    "\n",
    "# Initialize OpenVINO Runtime\n",
    "core = Core()\n",
    "\n",
    "local_dir = \"whisper-small-openvino-download\"\n",
    "\n",
    "# Path to OpenVINO IR model files\n",
    "model_xml = \"whisper-small-openvino-download/whisper_small/whisper_small_encoder.xml\"  # Update with the path to your XML file\n",
    "model_bin = model_xml.replace(\".xml\", \".bin\")  # Associated .bin file\n",
    "\n",
    "# Load and compile the model\n",
    "model = core.read_model(model=model_xml, weights=model_bin)\n",
    "compiled_model = core.compile_model(model=model, device_name=\"CPU\")\n",
    "\n",
    "# Get input and output info\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layer = compiled_model.output(0)\n",
    "\n",
    "print(f\"Input layer: {input_layer}\")\n",
    "print(f\"Output layer: {output_layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def preprocess_audio(audio_np, n_mels=80, frame_length=3000):\n",
    "    unneeded = audio_np.shape[0] % 80\n",
    "    if unneeded != 0:\n",
    "        audio_np = audio_np[:-unneeded]\n",
    "\n",
    "    # Convert the input to a PyTorch tensor\n",
    "    tensor = torch.tensor(audio_np, dtype=torch.float32)\n",
    "    \n",
    "    # Add a batch dimension\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "\n",
    "    # Calculate the number of frames\n",
    "    n_frames = tensor.shape[-1] // n_mels\n",
    "    tensor = tensor[:, :n_mels * n_frames].reshape(1, n_mels, n_frames)\n",
    "\n",
    "    # Adjust to match the required frame length (3000)\n",
    "    if tensor.shape[-1] < frame_length:\n",
    "        # Pad with zeros if shorter than required\n",
    "        padding = torch.zeros((1, n_mels, frame_length - tensor.shape[-1]), dtype=torch.float32)\n",
    "        tensor = torch.cat((tensor, padding), dim=-1)\n",
    "    elif tensor.shape[-1] > frame_length:\n",
    "        # Truncate if longer than required\n",
    "        tensor = tensor[:, :, :frame_length]\n",
    "\n",
    "    # Convert back to a NumPy array with contiguous memory layout\n",
    "    tensor_np = tensor.numpy()\n",
    "    return np.ascontiguousarray(tensor_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Exception from src/inference/src/cpp/core.cpp:95:\nException from src/frontends/ir/src/ir_deserializer.cpp:938:\nCannot create StringTensorUnpack layer StringTensorUnpack_83 id:3 from unsupported opset: extension\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m token_bin \u001b[38;5;241m=\u001b[39m token_xml\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.xml\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Associated .bin file\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#core.add_extension(\"path_to_extension_library\")\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load and compile the model\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_xml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_bin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m compiled_tokenizer \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mcompile_model(model\u001b[38;5;241m=\u001b[39mtokenizer, device_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_local_tokenizer\u001b[39m(xml_path, bin_path):\n",
      "File \u001b[0;32m~/Documents/cs/COMP0016/reading-star/venv/lib/python3.11/site-packages/openvino/_ov_api.py:534\u001b[0m, in \u001b[0;36mCore.read_model\u001b[0;34m(self, model, weights, config)\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Model(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mread_model(model, config\u001b[38;5;241m=\u001b[39mconfig))\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Model(\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Exception from src/inference/src/cpp/core.cpp:95:\nException from src/frontends/ir/src/ir_deserializer.cpp:938:\nCannot create StringTensorUnpack layer StringTensorUnpack_83 id:3 from unsupported opset: extension\n\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperTokenizer, AutoTokenizer\n",
    "from openvino_tokenizers import convert_tokenizer\n",
    "\n",
    "token_xml = \"whisper-small-openvino-download/ov-tokenizer/openvino_detokenizer.xml\"  # Update with the path to your XML file\n",
    "token_bin = token_xml.replace(\".xml\", \".bin\")  # Associated .bin file\n",
    "\n",
    "# Load and compile the model\n",
    "tokenizer = core.read_model(model=token_xml, weights=token_bin)\n",
    "compiled_tokenizer = core.compile_model(model=tokenizer, device_name=\"CPU\")\n",
    "\n",
    "def load_local_tokenizer(xml_path, bin_path):\n",
    "    \"\"\"\n",
    "    Load the tokenizer from local OpenVINO tokenizer model files.\n",
    "\n",
    "    Args:\n",
    "        xml_path (str): Path to the .xml file of the tokenizer.\n",
    "        bin_path (str): Path to the .bin file of the tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        Tokenizer: A tokenizer object compatible with OpenVINO.\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer(xml_path, bin_path)\n",
    "    return tokenizer\n",
    "\n",
    "#tokenizer = load_local_tokenizer(\"whisper-small-openvino/ov-tokenizer/openvino_detokenizer.xml\", \"whisper-small-openvino/ov-tokenizer/openvino_detokenizer.bin\")\n",
    "\n",
    "def decode_whisper_output(output_tensor):\n",
    "    \"\"\"\n",
    "    Decodes the output of the Whisper model into human-readable text.\n",
    "    \n",
    "    Args:\n",
    "        output_tensor (numpy.ndarray): The raw output from the Whisper model.\n",
    "        \n",
    "    Returns:\n",
    "        str: The decoded transcription.\n",
    "    \"\"\"\n",
    "    # Convert the tensor to a list of token IDs\n",
    "    token_ids = output_tensor.squeeze().tolist()\n",
    "    transcription = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and microphone initialized.\n",
      "\n",
      "Received 37888 samples for transcription\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'decode_whisper_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m result \u001b[38;5;241m=\u001b[39m compiled_model([audio_np])[output_layer]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Decode and print transcription\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m transcription \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_whisper_output\u001b[49m(result)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(transcription)\n\u001b[1;32m     64\u001b[0m text \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'decode_whisper_output' is not defined"
     ]
    }
   ],
   "source": [
    "# Define arguments as variables for notebook use\n",
    "non_english = False    # Use non-English model if True\n",
    "energy_threshold = 1000  # Energy level for mic detection\n",
    "record_timeout = 2.0     # Real-time recording in seconds\n",
    "phrase_timeout = 3.0     # Pause length between phrases for new line\n",
    "\n",
    "# Initialize variables\n",
    "phrase_time = None\n",
    "data_queue = Queue()\n",
    "recorder = sr.Recognizer()\n",
    "recorder.energy_threshold = energy_threshold\n",
    "recorder.dynamic_energy_threshold = False\n",
    "\n",
    "# Set up microphone source\n",
    "source = sr.Microphone(sample_rate=16000)\n",
    "\n",
    "\"\"\"\n",
    "# Load OpenVINO pipeline\n",
    "ov_pipeline = openvino_genai.WhisperPipeline(\"whisper-small-openvino\", device='CPU')\n",
    "\"\"\"\n",
    "# Initialize transcription list\n",
    "transcription = ['']\n",
    "\n",
    "# Adjust microphone for ambient noise\n",
    "with source:\n",
    "    recorder.adjust_for_ambient_noise(source)\n",
    "\n",
    "def record_callback(_, audio: sr.AudioData) -> None:\n",
    "    \"\"\"Threaded callback function to handle audio data.\"\"\"\n",
    "    data = audio.get_raw_data()\n",
    "    data_queue.put(data)\n",
    "\n",
    "# Start background recording\n",
    "recorder.listen_in_background(source, record_callback, phrase_time_limit=record_timeout)\n",
    "print(\"Model loaded and microphone initialized.\\n\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        now = datetime.utcnow()\n",
    "        if not data_queue.empty():\n",
    "            phrase_complete = False\n",
    "            if phrase_time and now - phrase_time > timedelta(seconds=phrase_timeout):\n",
    "                phrase_complete = True\n",
    "            phrase_time = now\n",
    "\n",
    "            audio_data = b''.join(data_queue.queue)\n",
    "            data_queue.queue.clear()\n",
    "\n",
    "\n",
    "            audio_np = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "            print(f\"Received {len(audio_np)} samples for transcription\")\n",
    "\n",
    "            # Preprocess audio\n",
    "            audio_np = preprocess_audio(audio_np)\n",
    "\n",
    "            # Run inference\n",
    "            result = compiled_model([audio_np])[output_layer]\n",
    "\n",
    "            # Decode and print transcription - by detokenizer\n",
    "            transcription = decode_whisper_output(result)\n",
    "            print(transcription)\n",
    "\n",
    "\n",
    "            text = result['text'].strip()\n",
    "\n",
    "            if phrase_complete:\n",
    "                transcription.append(text)\n",
    "            else:\n",
    "                transcription[-1] = text\n",
    "\n",
    "            os.system('cls' if os.name == 'nt' else 'clear')\n",
    "            for line in transcription:\n",
    "                print(line)\n",
    "            print('', end='', flush=True)\n",
    "        else:\n",
    "            sleep(0.25)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTranscription stopped by user.\")\n",
    "    print(\"\\nFinal Transcription:\")\n",
    "    for line in transcription:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuzzy matching to find closest match phrase in the current verse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonynkyi/Documents/cs/COMP0016/reading-star/venv/lib/python3.11/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got numpy.ndarray)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m source \u001b[38;5;241m=\u001b[39m sr\u001b[38;5;241m.\u001b[39mMicrophone(sample_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Load Whisper model\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m audio_model \u001b[38;5;241m=\u001b[39m \u001b[43mwhisper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbase.en\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Initialize transcription list\u001b[39;00m\n\u001b[1;32m     49\u001b[0m transcription \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/cs/COMP0016/reading-star/venv/lib/python3.11/site-packages/whisper/__init__.py:158\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, device, download_root, in_memory)\u001b[0m\n\u001b[1;32m    155\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m alignment_heads \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_alignment_heads\u001b[49m\u001b[43m(\u001b[49m\u001b[43malignment_heads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Documents/cs/COMP0016/reading-star/venv/lib/python3.11/site-packages/whisper/model.py:282\u001b[0m, in \u001b[0;36mWhisper.set_alignment_heads\u001b[0;34m(self, dump)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_alignment_heads\u001b[39m(\u001b[38;5;28mself\u001b[39m, dump: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m    279\u001b[0m     array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(\n\u001b[1;32m    280\u001b[0m         gzip\u001b[38;5;241m.\u001b[39mdecompress(base64\u001b[38;5;241m.\u001b[39mb85decode(dump)), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m    281\u001b[0m     )\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m--> 282\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_text_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_text_head\n\u001b[1;32m    284\u001b[0m     )\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malignment_heads\u001b[39m\u001b[38;5;124m\"\u001b[39m, mask\u001b[38;5;241m.\u001b[39mto_sparse(), persistent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got numpy.ndarray)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import speech_recognition as sr\n",
    "import whisper\n",
    "import torch\n",
    "from datetime import datetime, timedelta\n",
    "from queue import Queue\n",
    "from time import sleep\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Known lyrics for \"Twinkle, Twinkle, Little Star\"\n",
    "lyrics = {\n",
    "    \"Verse 1\": [\n",
    "        \"Twinkle, twinkle, little star\",\n",
    "        \"How I wonder what you are\",\n",
    "        \"Up above the world so high\",\n",
    "        \"Like a diamond in the sky\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Fuzzy matching function\n",
    "def find_closest_match(transcription, lyrics):\n",
    "    best_match = \"\"\n",
    "    highest_similarity = 0\n",
    "    for line in lyrics:\n",
    "        similarity = SequenceMatcher(None, transcription, line).ratio()\n",
    "        if similarity > highest_similarity:\n",
    "            highest_similarity = similarity\n",
    "            best_match = line\n",
    "    return best_match, highest_similarity\n",
    "\n",
    "# Initialize variables for speech recognition and Whisper\n",
    "energy_threshold = 1000  # Energy level for mic detection\n",
    "record_timeout = 2.0  # Real-time recording in seconds\n",
    "phrase_timeout = 3.0  # Pause length between phrases for new line\n",
    "phrase_time = None\n",
    "data_queue = Queue()\n",
    "recorder = sr.Recognizer()\n",
    "recorder.energy_threshold = energy_threshold\n",
    "recorder.dynamic_energy_threshold = False\n",
    "\n",
    "# Set up microphone source\n",
    "source = sr.Microphone(sample_rate=16000)\n",
    "\n",
    "# Initialize transcription list\n",
    "transcription = ['']\n",
    "\n",
    "# Adjust microphone for ambient noise\n",
    "with source:\n",
    "    recorder.adjust_for_ambient_noise(source)\n",
    "\n",
    "# Define a callback for audio data processing\n",
    "def record_callback(_, audio: sr.AudioData) -> None:\n",
    "    \"\"\"Threaded callback function to handle audio data.\"\"\"\n",
    "    data = audio.get_raw_data()\n",
    "    data_queue.put(data)\n",
    "\n",
    "# Start background recording\n",
    "recorder.listen_in_background(source, record_callback, phrase_time_limit=record_timeout)\n",
    "print(\"Model loaded and microphone initialized.\\n\")\n",
    "\n",
    "try:\n",
    "    current_verse = \"Verse 1\"  # Start with the first verse\n",
    "    while True:\n",
    "        now = datetime.utcnow()\n",
    "        if not data_queue.empty():\n",
    "            phrase_complete = False\n",
    "            if phrase_time and now - phrase_time > timedelta(seconds=phrase_timeout):\n",
    "                phrase_complete = True\n",
    "            phrase_time = now\n",
    "\n",
    "            # Combine audio data from queue\n",
    "            audio_data = b''.join(data_queue.queue)\n",
    "            data_queue.queue.clear()\n",
    "\n",
    "            # Convert audio data to the format Whisper expects\n",
    "            audio_np = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32)\n",
    "\n",
    "            # convert to shape [1, 80, 3000]\n",
    "            audio_np = preprocess_audio(audio_np)\n",
    "\n",
    "            # Perform transcription using Whisper\n",
    "            result = audio_model.transcribe(audio_np, fp16=torch.cuda.is_available())\n",
    "            recognized_text = result['text'].strip()\n",
    "\n",
    "            # Match the transcription to the current verse's lyrics\n",
    "            match, similarity = find_closest_match(recognized_text, lyrics[current_verse])\n",
    "\n",
    "            #if phrase_complete:\n",
    "             #   transcription.append(match if similarity > 0.7 else recognized_text)\n",
    "            #else:\n",
    "             #   transcription[-1] = match if similarity > 0.7 else recognized_text\n",
    "\n",
    "            # Print the transcription and matched lyrics\n",
    "            os.system('cls' if os.name == 'nt' else 'clear')\n",
    "            #print(\"Transcription (matched to lyrics):\\n\")\n",
    "            #for line in transcription:\n",
    "             #   print(line)\n",
    "            print(f\"\\nRecognized: {recognized_text}\")\n",
    "            print(f\"Best Match: {match} (Similarity: {similarity:.2f})\")\n",
    "        else:\n",
    "            sleep(0.25)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTranscription stopped by user.\")\n",
    "    print(\"\\nFinal Transcription:\")\n",
    "    for line in transcription:\n",
    "        print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
